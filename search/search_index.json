{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction Yann is a batteries included deep learning framework built in Pytorch. Inspired by Django and Rails, it aims to automate the tedious steps of a machine learning project so that you can focus on the (fun) hard parts. It makes it easy to quickly get a project started but also scales with you all the way to production. It could also be viewed as torch.nn extended, as it includes common new research modules that might be too experimental to be included in torch. Getting Started Install 1 pip install yann Create a Project 1 yann scaffold digit-recognition Yann Command Line Interface yann comes with a command line interface that makes it even easier to get started yann run train - yann train - Create a new project. yann serve - Serve a pretrained model through an API or web UI. yann scaffold - Print this help message. Project layout 1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Introduction"},{"location":"#introduction","text":"Yann is a batteries included deep learning framework built in Pytorch. Inspired by Django and Rails, it aims to automate the tedious steps of a machine learning project so that you can focus on the (fun) hard parts. It makes it easy to quickly get a project started but also scales with you all the way to production. It could also be viewed as torch.nn extended, as it includes common new research modules that might be too experimental to be included in torch.","title":"Introduction"},{"location":"#getting-started","text":"","title":"Getting Started"},{"location":"#install","text":"1 pip install yann","title":"Install"},{"location":"#create-a-project","text":"1 yann scaffold digit-recognition","title":"Create a Project"},{"location":"#yann-command-line-interface","text":"yann comes with a command line interface that makes it even easier to get started yann run train - yann train - Create a new project. yann serve - Serve a pretrained model through an API or web UI. yann scaffold - Print this help message.","title":"Yann Command Line Interface"},{"location":"#project-layout","text":"1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"alternatives/","text":"Alternatives Generic Frameworks Ignite https://github.com/pytorch/ignite Catalyst https://github.com/catalyst-team/catalyst Fast.ai https://github.com/fastai/fastai https://github.com/fastai/fastai_dev Bootstrap Pytorch https://github.com/Cadene/bootstrap.pytorch Torch-Lightning Skorch https://github.com/skorch-dev/skorch Torchbearer https://github.com/pytorchbearer/torchbearer Natural Language Processing AllenNLP Spacy Flair https://github.com/zalandoresearch/flair OpenNMT https://github.com/OpenNMT/OpenNMT-py StanfordNLP Fairseq https://github.com/pytorch/fairseq Huggingface Transformers https://github.com/huggingface/transformers PyText https://github.com/facebookresearch/pytext Translate https://github.com/pytorch/translate Computer Vision Torchvision https://github.com/pytorch/vision MMCV https://github.com/open-mmlab/mmcv MMDetection https://github.com/open-mmlab/mmdetection Detectron2 https://github.com/facebookresearch/detectron2 Kornia https://github.com/arraiyopensource/kornia/ cirtorch - cnnimageretrieval https://github.com/filipradenovic/cnnimageretrieval-pytorch Pretrained Models https://github.com/Cadene/pretrained-models.pytorch PyTorch Image Models https://github.com/rwightman/pytorch-image-models Reinforcement Learning Horizon https://github.com/facebookresearch/ReAgent Recommender Systems Spotlight https://github.com/maciejkula/spotlight Meta Learning Graphs Pytorch Geometric https://github.com/rusty1s/pytorch_geometric Probabilistic Programming https://github.com/pyro-ppl/pyro Hyperparameter Optimization AutoML Speech / Audio Pytorch-Kaldi https://github.com/mravanelli/pytorch-kaldi Medical MedicalTorch https://github.com/perone/medicaltorch Active Learning Bayesian Active Learning (Baal) https://github.com/ElementAI/baal","title":"Alternatives"},{"location":"alternatives/#alternatives","text":"","title":"Alternatives"},{"location":"alternatives/#generic-frameworks","text":"","title":"Generic Frameworks"},{"location":"alternatives/#ignite","text":"https://github.com/pytorch/ignite","title":"Ignite"},{"location":"alternatives/#catalyst","text":"https://github.com/catalyst-team/catalyst","title":"Catalyst"},{"location":"alternatives/#fastai","text":"https://github.com/fastai/fastai https://github.com/fastai/fastai_dev","title":"Fast.ai"},{"location":"alternatives/#bootstrap-pytorch","text":"https://github.com/Cadene/bootstrap.pytorch","title":"Bootstrap Pytorch"},{"location":"alternatives/#torch-lightning","text":"","title":"Torch-Lightning"},{"location":"alternatives/#skorch","text":"https://github.com/skorch-dev/skorch","title":"Skorch"},{"location":"alternatives/#torchbearer","text":"https://github.com/pytorchbearer/torchbearer","title":"Torchbearer"},{"location":"alternatives/#natural-language-processing","text":"","title":"Natural Language Processing"},{"location":"alternatives/#allennlp","text":"","title":"AllenNLP"},{"location":"alternatives/#spacy","text":"","title":"Spacy"},{"location":"alternatives/#flair","text":"https://github.com/zalandoresearch/flair","title":"Flair"},{"location":"alternatives/#opennmt","text":"https://github.com/OpenNMT/OpenNMT-py","title":"OpenNMT"},{"location":"alternatives/#stanfordnlp","text":"","title":"StanfordNLP"},{"location":"alternatives/#fairseq","text":"https://github.com/pytorch/fairseq","title":"Fairseq"},{"location":"alternatives/#huggingface-transformers","text":"https://github.com/huggingface/transformers","title":"Huggingface Transformers"},{"location":"alternatives/#pytext","text":"https://github.com/facebookresearch/pytext","title":"PyText"},{"location":"alternatives/#translate","text":"https://github.com/pytorch/translate","title":"Translate"},{"location":"alternatives/#computer-vision","text":"","title":"Computer Vision"},{"location":"alternatives/#torchvision","text":"https://github.com/pytorch/vision","title":"Torchvision"},{"location":"alternatives/#mmcv","text":"https://github.com/open-mmlab/mmcv","title":"MMCV"},{"location":"alternatives/#mmdetection","text":"https://github.com/open-mmlab/mmdetection","title":"MMDetection"},{"location":"alternatives/#detectron2","text":"https://github.com/facebookresearch/detectron2","title":"Detectron2"},{"location":"alternatives/#kornia","text":"https://github.com/arraiyopensource/kornia/","title":"Kornia"},{"location":"alternatives/#cirtorch-cnnimageretrieval","text":"https://github.com/filipradenovic/cnnimageretrieval-pytorch","title":"cirtorch - cnnimageretrieval"},{"location":"alternatives/#pretrained-models","text":"https://github.com/Cadene/pretrained-models.pytorch","title":"Pretrained Models"},{"location":"alternatives/#pytorch-image-models","text":"https://github.com/rwightman/pytorch-image-models","title":"PyTorch Image Models"},{"location":"alternatives/#reinforcement-learning","text":"","title":"Reinforcement Learning"},{"location":"alternatives/#horizon","text":"https://github.com/facebookresearch/ReAgent","title":"Horizon"},{"location":"alternatives/#recommender-systems","text":"","title":"Recommender Systems"},{"location":"alternatives/#spotlight","text":"https://github.com/maciejkula/spotlight","title":"Spotlight"},{"location":"alternatives/#meta-learning","text":"","title":"Meta Learning"},{"location":"alternatives/#graphs","text":"","title":"Graphs"},{"location":"alternatives/#pytorch-geometric","text":"https://github.com/rusty1s/pytorch_geometric","title":"Pytorch Geometric"},{"location":"alternatives/#probabilistic-programming","text":"https://github.com/pyro-ppl/pyro","title":"Probabilistic Programming"},{"location":"alternatives/#hyperparameter-optimization","text":"","title":"Hyperparameter Optimization"},{"location":"alternatives/#automl","text":"","title":"AutoML"},{"location":"alternatives/#speech-audio","text":"","title":"Speech / Audio"},{"location":"alternatives/#pytorch-kaldi","text":"https://github.com/mravanelli/pytorch-kaldi","title":"Pytorch-Kaldi"},{"location":"alternatives/#medical","text":"","title":"Medical"},{"location":"alternatives/#medicaltorch","text":"https://github.com/perone/medicaltorch","title":"MedicalTorch"},{"location":"alternatives/#active-learning","text":"","title":"Active Learning"},{"location":"alternatives/#bayesian-active-learning-baal","text":"https://github.com/ElementAI/baal","title":"Bayesian Active Learning (Baal)"},{"location":"baselines/","text":"Baseline Models","title":"Baseline Models"},{"location":"baselines/#baseline-models","text":"","title":"Baseline Models"},{"location":"callbacks/","text":"","title":"Callbacks"},{"location":"checklist/","text":"","title":"Checklist"},{"location":"classes/","text":"Classes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 from yann.data import Classes classes = Classes ([ 'apple' , 'banana' , 'fruit' ]) classes . names [ 0 ] classes . indices [ 'apple' ] classes . counts [ 0 ] classes . weights () classes . encode ([ 'apple' ]) classes . decode ([ 0 , 1 ]) \"apple\" in classes classes . ranked_decode ([ . 3 , . 5 ]) classes . one_hot_encode ([ 'apple' ]) classes . index_encode ([ 'apple' ]) classes . index_decode ([ 1 , 2 ]) Label Smoothing 1 2 3 from yann.data.classes import smooth smooth ( classes . encode ([ 'apple' ]), num_classes = len ( classes )) Class Weights","title":"Classes"},{"location":"classes/#classes","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 from yann.data import Classes classes = Classes ([ 'apple' , 'banana' , 'fruit' ]) classes . names [ 0 ] classes . indices [ 'apple' ] classes . counts [ 0 ] classes . weights () classes . encode ([ 'apple' ]) classes . decode ([ 0 , 1 ]) \"apple\" in classes classes . ranked_decode ([ . 3 , . 5 ]) classes . one_hot_encode ([ 'apple' ]) classes . index_encode ([ 'apple' ]) classes . index_decode ([ 1 , 2 ])","title":"Classes"},{"location":"classes/#label-smoothing","text":"1 2 3 from yann.data.classes import smooth smooth ( classes . encode ([ 'apple' ]), num_classes = len ( classes ))","title":"Label Smoothing"},{"location":"classes/#class-weights","text":"","title":"Class Weights"},{"location":"command-line/","text":"1 2 3 yann dependencies save yann dependencies upgrade yann dependencies","title":"Command line"},{"location":"common-bugs/","text":"Common Bugs Data Leakage Memory Leaks Broadcasting Over and Underflows","title":"Common Bugs"},{"location":"common-bugs/#common-bugs","text":"","title":"Common Bugs"},{"location":"common-bugs/#data-leakage","text":"","title":"Data Leakage"},{"location":"common-bugs/#memory-leaks","text":"","title":"Memory Leaks"},{"location":"common-bugs/#broadcasting","text":"","title":"Broadcasting"},{"location":"common-bugs/#over-and-underflows","text":"","title":"Over and Underflows"},{"location":"configuration/","text":"","title":"Configuration"},{"location":"dataset-wrappers/","text":"","title":"Dataset wrappers"},{"location":"evaluation/","text":"","title":"Evaluation"},{"location":"hyperparams/","text":"Hyper Parameters 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import yann from yann.params import HyperParams , Choice , Range class Params ( HyperParams ): dataset = 'MNIST' batch_size = 32 epochs = 10 optimizer : Choice (( 'SGD' , 'Adam' )) = 'SGD' learning_rate : Range ( . 01 , . 0001 ) = . 01 momentum = 0 seed = 1 # parse command line arguments params = Params . from_command () This will automatically generate a command line interface for your experiment, making it easy to try different configurations 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 usage : train_mnist . py [- h ] [- o { SGD , Adam }] [- lr LEARNING_RATE ] [- d DATASET ] [- bs BATCH_SIZE ] [- e EPOCHS ] [- m MOMENTUM ] [- s SEED ] optional arguments : - h , -- help show this help message and exit - o { SGD , Adam }, -- optimizer { SGD , Adam } optimizer ( default : SGD ) - lr LEARNING_RATE , -- learning_rate LEARNING_RATE learning_rate ( default : 0.01 ) - d DATASET , -- dataset DATASET dataset ( default : MNIST ) - bs BATCH_SIZE , -- batch_size BATCH_SIZE batch_size ( default : 32 ) - e EPOCHS , -- epochs EPOCHS epochs ( default : 10 ) - m MOMENTUM , -- momentum MOMENTUM momentum ( default : 0 ) - s SEED , -- seed SEED seed ( default : 1 ) Fields Validation 1 params . validate () Watching for changes 1 2 3 4 params . on_change ( lambda k , v : print ( f \"changing {k} to {v}\" )) params . learning_rate = 5 # > changing learning_rate to 5 Sampling and Parameter Grids 1 2 3 4 Params . sample () Params . grid () Saving 1 2 params . save ( 'params.yml' ) params . save ( 'params.json' ) Loading 1 2 params . load ( 'params.yml' ) params . load ( 'params.json' ) Function Binding 1 2 3 4 5 6 7 8 9 10 11 12 13 class Params ( HyperParams ): model = 'resnet50' @params.bind () def train ( model , batch_size = 32 , optimizer = 'Adam' ): pass # train using the default parameters train () # override the parameters and update the params train ( model = 'seresnext50' )","title":"Hyper Parameters"},{"location":"hyperparams/#hyper-parameters","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import yann from yann.params import HyperParams , Choice , Range class Params ( HyperParams ): dataset = 'MNIST' batch_size = 32 epochs = 10 optimizer : Choice (( 'SGD' , 'Adam' )) = 'SGD' learning_rate : Range ( . 01 , . 0001 ) = . 01 momentum = 0 seed = 1 # parse command line arguments params = Params . from_command () This will automatically generate a command line interface for your experiment, making it easy to try different configurations 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 usage : train_mnist . py [- h ] [- o { SGD , Adam }] [- lr LEARNING_RATE ] [- d DATASET ] [- bs BATCH_SIZE ] [- e EPOCHS ] [- m MOMENTUM ] [- s SEED ] optional arguments : - h , -- help show this help message and exit - o { SGD , Adam }, -- optimizer { SGD , Adam } optimizer ( default : SGD ) - lr LEARNING_RATE , -- learning_rate LEARNING_RATE learning_rate ( default : 0.01 ) - d DATASET , -- dataset DATASET dataset ( default : MNIST ) - bs BATCH_SIZE , -- batch_size BATCH_SIZE batch_size ( default : 32 ) - e EPOCHS , -- epochs EPOCHS epochs ( default : 10 ) - m MOMENTUM , -- momentum MOMENTUM momentum ( default : 0 ) - s SEED , -- seed SEED seed ( default : 1 )","title":"Hyper Parameters"},{"location":"hyperparams/#fields","text":"","title":"Fields"},{"location":"hyperparams/#validation","text":"1 params . validate ()","title":"Validation"},{"location":"hyperparams/#watching-for-changes","text":"1 2 3 4 params . on_change ( lambda k , v : print ( f \"changing {k} to {v}\" )) params . learning_rate = 5 # > changing learning_rate to 5","title":"Watching for changes"},{"location":"hyperparams/#sampling-and-parameter-grids","text":"1 2 3 4 Params . sample () Params . grid ()","title":"Sampling and Parameter Grids"},{"location":"hyperparams/#saving","text":"1 2 params . save ( 'params.yml' ) params . save ( 'params.json' )","title":"Saving"},{"location":"hyperparams/#loading","text":"1 2 params . load ( 'params.yml' ) params . load ( 'params.json' )","title":"Loading"},{"location":"hyperparams/#function-binding","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 class Params ( HyperParams ): model = 'resnet50' @params.bind () def train ( model , batch_size = 32 , optimizer = 'Adam' ): pass # train using the default parameters train () # override the parameters and update the params train ( model = 'seresnext50' )","title":"Function Binding"},{"location":"inference/","text":"Inference Batch Inference Low Latency Inference","title":"Inference"},{"location":"inference/#inference","text":"","title":"Inference"},{"location":"inference/#batch-inference","text":"","title":"Batch Inference"},{"location":"inference/#low-latency-inference","text":"","title":"Low Latency Inference"},{"location":"initialization/","text":"","title":"Initialization"},{"location":"logging/","text":"","title":"Logging"},{"location":"performance-tips/","text":"Performance GPU Utilization CPU Utilization Disk IO Network IO CPU => GPU Memory Bandwidth Distributed Training Inference","title":"Performance"},{"location":"performance-tips/#performance","text":"","title":"Performance"},{"location":"performance-tips/#gpu-utilization","text":"","title":"GPU Utilization"},{"location":"performance-tips/#cpu-utilization","text":"","title":"CPU Utilization"},{"location":"performance-tips/#disk-io","text":"","title":"Disk IO"},{"location":"performance-tips/#network-io","text":"","title":"Network IO"},{"location":"performance-tips/#cpu-gpu-memory-bandwidth","text":"","title":"CPU =&gt; GPU Memory Bandwidth"},{"location":"performance-tips/#distributed","text":"","title":"Distributed"},{"location":"performance-tips/#training","text":"","title":"Training"},{"location":"performance-tips/#inference","text":"","title":"Inference"},{"location":"pipelines/","text":"","title":"Pipelines"},{"location":"profiling/","text":"","title":"Profiling"},{"location":"registry/","text":"Registry Yann provides an object registry that enables it to resolve instances from strings. You can register your own objects to make them discoverable by the system. Resolving by name 1 2 3 4 import yann resnet = yann . resolve . model ( 'resnet18' , pretrained = True ) Registering your own objects 1 2 3 4 5 import yann @yann.register.loss def custom_loss (): pass","title":"Registry"},{"location":"registry/#registry","text":"Yann provides an object registry that enables it to resolve instances from strings. You can register your own objects to make them discoverable by the system.","title":"Registry"},{"location":"registry/#resolving-by-name","text":"1 2 3 4 import yann resnet = yann . resolve . model ( 'resnet18' , pretrained = True )","title":"Resolving by name"},{"location":"registry/#registering-your-own-objects","text":"1 2 3 4 5 import yann @yann.register.loss def custom_loss (): pass","title":"Registering your own objects"},{"location":"reports/","text":"Reports Generate Experiment Reports","title":"Reports"},{"location":"reports/#reports","text":"","title":"Reports"},{"location":"reports/#generate-experiment-reports","text":"","title":"Generate Experiment Reports"},{"location":"serving/","text":"Serving Models 1 from yann.server import ImageRecognitionServer API GraphQL Web UI Runtimes","title":"Serving Models"},{"location":"serving/#serving-models","text":"1 from yann.server import ImageRecognitionServer","title":"Serving Models"},{"location":"serving/#api","text":"","title":"API"},{"location":"serving/#graphql","text":"","title":"GraphQL"},{"location":"serving/#web-ui","text":"","title":"Web UI"},{"location":"serving/#runtimes","text":"","title":"Runtimes"},{"location":"shape-inference/","text":"","title":"Shape inference"},{"location":"stack/","text":"Stack Model Stack is an alternative to Pytorch's nn.Sequential , 1 from yann.modules import Stack","title":"Stack Model"},{"location":"stack/#stack-model","text":"Stack is an alternative to Pytorch's nn.Sequential , 1 from yann.modules import Stack","title":"Stack Model"},{"location":"testing/","text":"Testing and Validation 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import torch from yann.testing import check_tensor t = torch . randn ( 32 , 3 , 224 , 224 ) t2 = t check_tensor ( t , device = 'cpu' , gte = 0 , # greater than or equal lte = 1 , # less than or equal share_memory = t2 , # using the same storage shape = ( None , 3 , 224 , 224 ), # dimensions match anomalies = True , # check for infs or NaNs )","title":"Testing and Validation"},{"location":"testing/#testing-and-validation","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import torch from yann.testing import check_tensor t = torch . randn ( 32 , 3 , 224 , 224 ) t2 = t check_tensor ( t , device = 'cpu' , gte = 0 , # greater than or equal lte = 1 , # less than or equal share_memory = t2 , # using the same storage shape = ( None , 3 , 224 , 224 ), # dimensions match anomalies = True , # check for infs or NaNs )","title":"Testing and Validation"},{"location":"training/","text":"1 2 3 4 5 6 7 8 from yann.train import Trainer train = Trainer ( model = 'resnet18' , dataset = 'MNIST' , optimizer = 'AdamW' , loss = 'cross_entropy' ) Register callbacks 1 2 3 @train.on ( 'epoch_end' ) def sync_data (): pass Implement custom step logic 1 2 3 4 5 6 7 8 for e in train . epochs ( 4 ): for inputs , targets in train . batches (): train . optimizer . zero_grad () outputs = train . model ( inputs ) loss = train . loss ( outputs , targets ) loss . backwards () train . checkpoint () Checkpointing 1 train . checkpoint () Continue where you left off 1 train . load_checkpoint ( 'latest' ) History 1 train . history . plot () Functional Interface 1 2 3 4 5 from yann.train import train for epoch in range ( 10 ): for _ in train ( model , loader , optimizer , loss = 'cross_entropy' , device = 'cuda' ): pass","title":"Training"},{"location":"training/#register-callbacks","text":"1 2 3 @train.on ( 'epoch_end' ) def sync_data (): pass","title":"Register callbacks"},{"location":"training/#implement-custom-step-logic","text":"1 2 3 4 5 6 7 8 for e in train . epochs ( 4 ): for inputs , targets in train . batches (): train . optimizer . zero_grad () outputs = train . model ( inputs ) loss = train . loss ( outputs , targets ) loss . backwards () train . checkpoint ()","title":"Implement custom step logic"},{"location":"training/#checkpointing","text":"1 train . checkpoint ()","title":"Checkpointing"},{"location":"training/#continue-where-you-left-off","text":"1 train . load_checkpoint ( 'latest' )","title":"Continue where you left off"},{"location":"training/#history","text":"1 train . history . plot ()","title":"History"},{"location":"training/#functional-interface","text":"1 2 3 4 5 from yann.train import train for epoch in range ( 10 ): for _ in train ( model , loader , optimizer , loss = 'cross_entropy' , device = 'cuda' ): pass","title":"Functional Interface"},{"location":"visualization/","text":"","title":"Visualization"},{"location":"data/io/","text":"IO Saving and Loading 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import yann yann . save ( x , 'data.yml' ) yann . save ( x , 'data.json' ) yann . save ( x , 'data.th' ) yann . save ( x , 'data.pkl' ) yann . save ( x , 'data.csv' ) yann . save ( x , 'data.parq' ) yann . save ( x , 'data.parquet' ) yann . save ( x , 's3://bucket-name/data.parquet' ) x = yann . load ( 'data.npz' ) Downloader Serialization Syncing","title":"IO"},{"location":"data/io/#io","text":"","title":"IO"},{"location":"data/io/#saving-and-loading","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import yann yann . save ( x , 'data.yml' ) yann . save ( x , 'data.json' ) yann . save ( x , 'data.th' ) yann . save ( x , 'data.pkl' ) yann . save ( x , 'data.csv' ) yann . save ( x , 'data.parq' ) yann . save ( x , 'data.parquet' ) yann . save ( x , 's3://bucket-name/data.parquet' ) x = yann . load ( 'data.npz' )","title":"Saving and Loading"},{"location":"data/io/#downloader","text":"","title":"Downloader"},{"location":"data/io/#serialization","text":"","title":"Serialization"},{"location":"data/io/#syncing","text":"","title":"Syncing"},{"location":"data/storage/","text":"Storage Key Value Stores LMDB Approximate Nearest Neighbor Faiss Annoy Columnar Parquet","title":"Storage"},{"location":"data/storage/#storage","text":"","title":"Storage"},{"location":"data/storage/#key-value-stores","text":"","title":"Key Value Stores"},{"location":"data/storage/#lmdb","text":"","title":"LMDB"},{"location":"data/storage/#approximate-nearest-neighbor","text":"","title":"Approximate Nearest Neighbor"},{"location":"data/storage/#faiss","text":"","title":"Faiss"},{"location":"data/storage/#annoy","text":"","title":"Annoy"},{"location":"data/storage/#columnar","text":"","title":"Columnar"},{"location":"data/storage/#parquet","text":"","title":"Parquet"},{"location":"guides/best-practices/","text":"Best Practices Reproducible Experiments Seeds Git Env Docker Pinned Requirements Logging Experiment Tracking Baseline Evaluation First Change one thing at a time Avoiding Drift Between Development and Production Immutable Data Testing Human Baseline / Data Annotation Build a hand annotated held out set by annotating the data. Measure your performance against the test set. Evaluate your model against your set. Error Analysis Overfitting a Batch Starting with a smaller dataset Starting with the simplest model Hyper Parameter Tunning Iteration Speed Usable by Others Documentation Website Common Commands Fine Tunning Caching Model Outputs Training in GPU Data Management Versioning Datasets Tracking Changes Production Monitoring Shadow Deployments Feedback Load Testing Batching Continuous Integration Automate Training Other Resources A Recipe for Training Neural Networks Hacker's guide to Neural Networks Full Stack Deep Learning Videos Troubleshooting Deep Neural Networks - A Field Guide to Fixing Your Model Hidden Technical Debt in Machine Learning Systems Andrew Ng's Machine LEarning Yearning Twitter Thread What should I do when my neural network doesn't learn?","title":"Best Practices"},{"location":"guides/best-practices/#best-practices","text":"","title":"Best Practices"},{"location":"guides/best-practices/#reproducible-experiments","text":"","title":"Reproducible Experiments"},{"location":"guides/best-practices/#seeds","text":"","title":"Seeds"},{"location":"guides/best-practices/#git-env","text":"","title":"Git Env"},{"location":"guides/best-practices/#docker","text":"","title":"Docker"},{"location":"guides/best-practices/#pinned-requirements","text":"","title":"Pinned Requirements"},{"location":"guides/best-practices/#logging","text":"","title":"Logging"},{"location":"guides/best-practices/#experiment-tracking","text":"","title":"Experiment Tracking"},{"location":"guides/best-practices/#baseline","text":"","title":"Baseline"},{"location":"guides/best-practices/#evaluation-first","text":"","title":"Evaluation First"},{"location":"guides/best-practices/#change-one-thing-at-a-time","text":"","title":"Change one thing at a time"},{"location":"guides/best-practices/#avoiding-drift-between-development-and-production","text":"","title":"Avoiding Drift Between Development and Production"},{"location":"guides/best-practices/#immutable-data","text":"","title":"Immutable Data"},{"location":"guides/best-practices/#testing","text":"","title":"Testing"},{"location":"guides/best-practices/#human-baseline-data-annotation","text":"Build a hand annotated held out set by annotating the data. Measure your performance against the test set. Evaluate your model against your set.","title":"Human Baseline / Data Annotation"},{"location":"guides/best-practices/#error-analysis","text":"","title":"Error Analysis"},{"location":"guides/best-practices/#overfitting-a-batch","text":"","title":"Overfitting a Batch"},{"location":"guides/best-practices/#starting-with-a-smaller-dataset","text":"","title":"Starting with a smaller dataset"},{"location":"guides/best-practices/#starting-with-the-simplest-model","text":"","title":"Starting with the simplest model"},{"location":"guides/best-practices/#hyper-parameter-tunning","text":"","title":"Hyper Parameter Tunning"},{"location":"guides/best-practices/#iteration-speed","text":"","title":"Iteration Speed"},{"location":"guides/best-practices/#usable-by-others","text":"","title":"Usable by Others"},{"location":"guides/best-practices/#documentation","text":"","title":"Documentation"},{"location":"guides/best-practices/#website","text":"","title":"Website"},{"location":"guides/best-practices/#common-commands","text":"","title":"Common Commands"},{"location":"guides/best-practices/#fine-tunning","text":"","title":"Fine Tunning"},{"location":"guides/best-practices/#caching-model-outputs","text":"","title":"Caching Model Outputs"},{"location":"guides/best-practices/#training-in-gpu","text":"","title":"Training in GPU"},{"location":"guides/best-practices/#data-management","text":"","title":"Data Management"},{"location":"guides/best-practices/#versioning-datasets","text":"","title":"Versioning Datasets"},{"location":"guides/best-practices/#tracking-changes","text":"","title":"Tracking Changes"},{"location":"guides/best-practices/#production","text":"","title":"Production"},{"location":"guides/best-practices/#monitoring","text":"","title":"Monitoring"},{"location":"guides/best-practices/#shadow-deployments","text":"","title":"Shadow Deployments"},{"location":"guides/best-practices/#feedback","text":"","title":"Feedback"},{"location":"guides/best-practices/#load-testing","text":"","title":"Load Testing"},{"location":"guides/best-practices/#batching","text":"","title":"Batching"},{"location":"guides/best-practices/#continuous-integration","text":"","title":"Continuous Integration"},{"location":"guides/best-practices/#automate-training","text":"","title":"Automate Training"},{"location":"guides/best-practices/#other-resources","text":"A Recipe for Training Neural Networks Hacker's guide to Neural Networks Full Stack Deep Learning Videos Troubleshooting Deep Neural Networks - A Field Guide to Fixing Your Model Hidden Technical Debt in Machine Learning Systems Andrew Ng's Machine LEarning Yearning Twitter Thread What should I do when my neural network doesn't learn?","title":"Other Resources"},{"location":"guides/debugging/","text":"","title":"Debugging"},{"location":"guides/transfer-learning/","text":"","title":"Transfer learning"},{"location":"integrations/cometml/","text":"","title":"Cometml"},{"location":"integrations/slack/","text":"","title":"Slack"},{"location":"integrations/weights-and-biases/","text":"","title":"Weights and biases"},{"location":"tutorials/custom-training-logic/","text":"Trainer with custom Logic 1","title":"Trainer with custom Logic"},{"location":"tutorials/custom-training-logic/#trainer-with-custom-logic","text":"1","title":"Trainer with custom Logic"},{"location":"tutorials/mnist/","text":"LeNet in 30 Seconds Hyper Parameters to command line Step one of any good experiment is defining the parameters 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import yann from yann.params import HyperParams , Choice , Range class Params ( HyperParams ): dataset = 'MNIST' batch_size = 32 epochs = 10 optimizer : Choice (( 'SGD' , 'Adam' )) = 'SGD' learning_rate : Range ( . 01 , . 0001 ) = . 01 momentum = 0 seed = 1 # parse command line arguments params = Params . from_command () This will automatically generate a command line interface for your experiment, making it easy to try different configurations 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 usage : train_mnist . py [- h ] [- o { SGD , Adam }] [- lr LEARNING_RATE ] [- d DATASET ] [- bs BATCH_SIZE ] [- e EPOCHS ] [- m MOMENTUM ] [- s SEED ] optional arguments : - h , -- help show this help message and exit - o { SGD , Adam }, -- optimizer { SGD , Adam } optimizer ( default : SGD ) - lr LEARNING_RATE , -- learning_rate LEARNING_RATE learning_rate ( default : 0.01 ) - d DATASET , -- dataset DATASET dataset ( default : MNIST ) - bs BATCH_SIZE , -- batch_size BATCH_SIZE batch_size ( default : 32 ) - e EPOCHS , -- epochs EPOCHS epochs ( default : 10 ) - m MOMENTUM , -- momentum MOMENTUM momentum ( default : 0 ) - s SEED , -- seed SEED seed ( default : 1 ) next we'll set the seed to make our experiment reproducible 1 2 # set random, numpy and pytorch seeds in one call yann . seed ( params . seed ) Model Definition with Shape Inference Calculating the input shape of your layers can be a pain, with the Infer() module we can let the framework do it for us. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 from torch import nn from yann.modules import Stack , Infer lenet = Stack ( Infer ( nn . Conv2d , 10 , kernel_size = 5 ), nn . MaxPool2d ( 2 ), nn . ReLU (), Infer ( nn . Conv2d , 20 , kernel_size = 5 ), nn . MaxPool2d ( 2 ), nn . ReLU (), nn . Flatten (), Infer ( nn . Linear , 50 ), nn . ReLU (), Infer ( nn . Linear , 10 ), activation = nn . LogSoftmax ( dim = 1 ) ) # run a forward pass to infer input shapes using `Infer` modules lenet ( torch . rand ( 1 , 1 , 28 , 28 )) Configure a Trainer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 import torch from torch import nn from torchvision import transforms import yann from yann.train import Trainer # use the registry to resolve optimizer name to an optimizer class optimizer = yann . resolve . optimizer ( params . optimizer , yann . trainable ( lenet . parameters ()), momentum = params . momentum , lr = params . learning_rate ) train = Trainer ( model = lenet , optimizer = params . optimizer , dataset = params . dataset , batch_size = params . batch_size , transform = transforms . Compose ([ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)) ]), loss = 'nll_loss' , metrics = ( 'accuracy' , 'top_3_accuracy' ), ) train ( params . epochs ) # save checkpoint train . checkpoint () # plot the loss curve train . history . plot ()","title":"LeNet in 30 Seconds"},{"location":"tutorials/mnist/#lenet-in-30-seconds","text":"","title":"LeNet in 30 Seconds"},{"location":"tutorials/mnist/#hyper-parameters-to-command-line","text":"Step one of any good experiment is defining the parameters 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import yann from yann.params import HyperParams , Choice , Range class Params ( HyperParams ): dataset = 'MNIST' batch_size = 32 epochs = 10 optimizer : Choice (( 'SGD' , 'Adam' )) = 'SGD' learning_rate : Range ( . 01 , . 0001 ) = . 01 momentum = 0 seed = 1 # parse command line arguments params = Params . from_command () This will automatically generate a command line interface for your experiment, making it easy to try different configurations 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 usage : train_mnist . py [- h ] [- o { SGD , Adam }] [- lr LEARNING_RATE ] [- d DATASET ] [- bs BATCH_SIZE ] [- e EPOCHS ] [- m MOMENTUM ] [- s SEED ] optional arguments : - h , -- help show this help message and exit - o { SGD , Adam }, -- optimizer { SGD , Adam } optimizer ( default : SGD ) - lr LEARNING_RATE , -- learning_rate LEARNING_RATE learning_rate ( default : 0.01 ) - d DATASET , -- dataset DATASET dataset ( default : MNIST ) - bs BATCH_SIZE , -- batch_size BATCH_SIZE batch_size ( default : 32 ) - e EPOCHS , -- epochs EPOCHS epochs ( default : 10 ) - m MOMENTUM , -- momentum MOMENTUM momentum ( default : 0 ) - s SEED , -- seed SEED seed ( default : 1 ) next we'll set the seed to make our experiment reproducible 1 2 # set random, numpy and pytorch seeds in one call yann . seed ( params . seed )","title":"Hyper Parameters to command line"},{"location":"tutorials/mnist/#model-definition-with-shape-inference","text":"Calculating the input shape of your layers can be a pain, with the Infer() module we can let the framework do it for us. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 from torch import nn from yann.modules import Stack , Infer lenet = Stack ( Infer ( nn . Conv2d , 10 , kernel_size = 5 ), nn . MaxPool2d ( 2 ), nn . ReLU (), Infer ( nn . Conv2d , 20 , kernel_size = 5 ), nn . MaxPool2d ( 2 ), nn . ReLU (), nn . Flatten (), Infer ( nn . Linear , 50 ), nn . ReLU (), Infer ( nn . Linear , 10 ), activation = nn . LogSoftmax ( dim = 1 ) ) # run a forward pass to infer input shapes using `Infer` modules lenet ( torch . rand ( 1 , 1 , 28 , 28 ))","title":"Model Definition with Shape Inference"},{"location":"tutorials/mnist/#configure-a-trainer","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 import torch from torch import nn from torchvision import transforms import yann from yann.train import Trainer # use the registry to resolve optimizer name to an optimizer class optimizer = yann . resolve . optimizer ( params . optimizer , yann . trainable ( lenet . parameters ()), momentum = params . momentum , lr = params . learning_rate ) train = Trainer ( model = lenet , optimizer = params . optimizer , dataset = params . dataset , batch_size = params . batch_size , transform = transforms . Compose ([ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)) ]), loss = 'nll_loss' , metrics = ( 'accuracy' , 'top_3_accuracy' ), ) train ( params . epochs ) # save checkpoint train . checkpoint () # plot the loss curve train . history . plot ()","title":"Configure a Trainer"},{"location":"tutorials/transfer-learning/","text":"","title":"Transfer learning"}]}