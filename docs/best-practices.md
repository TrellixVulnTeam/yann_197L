# Best Practices

## Reproducible Experiments

### Seeds

### Git Env

### Docker

### Pinned Requirements

### Logging


## Experiment Tracking


## Baseline


## Evaluation First

## Change one thing at a time


## Avoiding Drift Between Development and Production


## Immutable Data


## Testing


## Human Baseline / Data Annotation

Build a hand annotated held out set by annotating the data. 
Measure your performance against the test set. Evaluate your model against your set.

## Error Analysis


## Overfitting a Batch

## Starting with a smaller dataset


## Starting with the simplest model


## Hyper Parameter Tunning

## Iteration Speed

## Usable by Others

### Documentation

### Website

### Common Commands

## Fine Tunning

### Caching Model Outputs

### Training in GPU

## Data Management

### Versioning Datasets

### Tracking Changes


## Production

### Monitoring

### Shadow Deployments

### Feedback

### Load Testing

### Batching

### Continuous Integration

### Automate Training

## Other Resources

- [A Recipe for Training Neural Networks](http://karpathy.github.io/2019/04/25/recipe/)
- [Hacker's guide to Neural Networks](http://karpathy.github.io/neuralnets/)
- [Full Stack Deep Learning](https://fullstackdeeplearning.com/march2019)
  - [Videos](https://www.youtube.com/channel/UCVchfoB65aVtQiDITbGq2LQ/featured)
- [Troubleshooting Deep Neural Networks - A Field Guide to Fixing Your Model](http://josh-tobin.com/troubleshooting-deep-neural-networks.html)
- [Hidden Technical Debt in Machine Learning Systems](https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems)
- [Andrew Ng's Machine LEarning Yearning](https://github.com/ajaymache/machine-learning-yearning)